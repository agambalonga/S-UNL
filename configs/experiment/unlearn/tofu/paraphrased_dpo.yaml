# @package _global_

defaults:
  - override /model: Llama-3.2-1B-Instruct
  - override /trainer: DPO
  - override /data: unlearn
  - override /data/datasets@data.forget: TOFU_QA_forget_augmented_idk
  - override /data/datasets@data.retain: TOFU_QA_retain
  - override /eval: tofu

model:
  model_args:
    pretrained_model_name_or_path: open-unlearning/tofu_Llama-3.2-1B-Instruct_full

forget_split: forget10
retain_split: retain90
retain_logs_path: null

eval:
  tofu:
    forget_split: ${forget_split}
    retain_logs_path: ${retain_logs_path}
    overwrite: true
    batch_size: 2  # Ridotto per DPO (memory intensive)

data:
  anchor: forget
  forget:
    TOFU_QA_forget_augmented_idk:
      args:
        json_path: data/tofu_${forget_split}_augmented.json
        idk_path: ./data/idk.jsonl
        question_key: question
        answer_key: answer
        max_length: 512
        num_paraphrases_per_question: 5  # ⬅️ Usa solo 5 parafrasi per ogni domanda
        include_original: true  # ⬅️ Include anche la domanda originale
  retain:
    TOFU_QA_retain:
      args:
        hf_args:
          name: ${retain_split}

trainer:
  args:
    warmup_epochs: 1.0 # custom parameter
    # learning_rate: 1e-5
    weight_decay: 0.01
    num_train_epochs: 10
    per_device_eval_batch_size: 4  # Ridotto da 16 per evitare OOM
    eval_on_start: False

task_name: ???